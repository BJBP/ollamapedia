# Ollama Streaming Web Page

[![en](https://img.shields.io/badge/lang-en-blue.svg)](README.md)
[![es](https://img.shields.io/badge/lang-es-green.svg)](README_ES.md)

[English](README.md) | [Espa√±ol](README_ES.md)

A simple web page that connects to Ollama using the phi4-mini:latest model and uses streaming responses.

This project provides a basic web interface for interacting with the Ollama language model. It allows users to enter a prompt and receive a real-time, chunked response from the model.

## Table of Contents

*   [About](#about)
*   [Installation](#installation)
*   [Usage](#usage)
*   [Contributing](#contributing)
*   [License](#license)

## About

This project demonstrates how to create a web page that connects to the Ollama language model and displays streaming responses. It uses HTML, CSS, and JavaScript to create a simple and interactive user interface.

## Installation

1.  Make sure you have Ollama installed and running.
2.  Clone the repository:

```bash
git clone https://github.com/your-username/your-project.git
```

3.  Navigate to the project directory:

```bash
cd your-project
```

4.  Open the `index.html` file in your browser.

## Usage

1.  Type your prompt in the input field and click the "Send" button.
2.  The response from Ollama will be displayed in the output div.

## Contributing

Contributions are welcome! Please submit a pull request with your changes.

## License

MIT